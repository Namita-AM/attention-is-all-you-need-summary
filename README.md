# Transformers - "Attention Is All You Need"  
*A visual walkthrough of the 2017 paper by Vaswani et al.*

This repository contains a slide-based, visual summary of the landmark paper that introduced the **Transformer** architecture; now the backbone of modern language models like **BERT**, **GPT** and more.

## What's Inside

- Overview of sequence modeling and limitations of RNNs
- Self-attention explained with intuitive examples
- Encoder and decoder architecture breakdown
- Input embeddings, position encoding, and residual connections
- Masked multi-head attention and parallelization advantages
- Key results from the original paper (BLEU scores, training cost)

This project is part of my ongoing effort to:
- Build a visual reference collection of influential machine learning papers  
- Strengthen conceptual understanding of NLP architectures  
- Communicate complex ideas clearly using design-first presentations
